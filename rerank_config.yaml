model:
  mode: cross
  cross_name: BAAI/bge-reranker-v2-m3
  embedding_name: Qwen/Qwen3-Embedding-8B
  batch_size_cross: 32
  batch_size_bi: 64
  normalize_embeddings: true
  trust_remote_code: true
  # Advanced options for direct Transformers models (Qwen)
  dtype: "bfloat16"             # bf16 recommended on RTX 5090
  use_flash_attention: true     # if flash-attn available, otherwise ignored
  max_tokens: 16384             # 16k safe on 5090; 32k possible but reduce batch
  output_dimension: 1536        # e.g. 1024 (PCA down-projection). Leave null otherwise.

huggingface:
  token: null
  cache_dir: null
  model_dir: /app/models
  prefetch:
    - Qwen/Qwen3-Embedding-8B
    - BAAI/bge-reranker-v2-m3

server:
  host: 0.0.0.0
  port: 8000
  cors_origins: ["*"]
  api_keys:
    - change-me-123
  warmup:
    enabled: true
    load:
      cross: true       # load the cross-encoder (reranker)
      bi: false         # load the bi-encoder (if using "bi" mode)
      embedding: true   # load the embedding model used by /v1/encode
    texts: ["warmup", "test"]  # small texts for a forward pass

logging:
  level: DEBUG
  format: json    # json or text
  file: /logs/rerank.log      # null -> stdout (container logs)